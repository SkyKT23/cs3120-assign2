{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SkyKT23/cs3120-assign2/blob/main/lab1_text_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdVY-7cDJmaC"
      },
      "source": [
        "# DSML4220 - Lab 1: Working with Text Data\n",
        "\n",
        "In this notebook we'll explore the Airline Tweet dataset and implement the simplest _\"model\"_ that we can come up (we use quotes here to refer to _model_ because it likely doesn't look like what you would expect, given that it deals with text/language).  Ultimately, the model will predict whether a tweet is a) positive, b) neutral, or c) negative.\n",
        "\n",
        "To run this notebook in Google Colab or on Kaggle, click one of the following links.  When you are done you will save your (completed) notebook in Kaggle, Colab, or a GitHub gist, and then submit the link to it in Canvas.\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sgeinitz/DSML4220/blob/main/lab1_text_data.ipynb)\n",
        "\n",
        "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/sgeinitz/DSML4220/blob/main/lab1_text_data.ipynb)\n",
        "\n",
        "Table of Contents\n",
        "* [0: Python Review](#0-python-review)\n",
        "* [1: Loading Data](#1-loading-dataset)\n",
        "* [2: Develop Simple Model](#2-develop-simple-model)\n",
        "* [3: Evaluate Simple Model](#3-evaluate-simple-model)\n",
        "* [4: Tokenization via Stemming](#4-tokenization-via-stemming)\n",
        "* [5: Tokenization via Lemmatization](#5-tokenization-via-lemmatization)\n",
        "* [6: Vectorization (and a 'real' ML Model)](#6-vectorization)\n",
        "\n",
        "All of the labs will have a few questions that you will need to answer (perhaps after modifying/writing some code). Ideally you work through this notebook from start to finish and answer the questions as you go. But to help ensure that no questions are left unanswered, they are linked to here:\n",
        "* [Q1](#q1)\n",
        "* [Q2](#q1)\n",
        "* [Q3](#q3)\n",
        "* [Q4](#q4)\n",
        "* [Q5](#q5)\n",
        "***\n",
        "Note that this notebook does not look like most Jupyter notebooks you will see. That's because you will typically load all of the Python modules/libraries that you need at the beginning in the very first code cell. However, in order for us to see which library we're using where, we'll load them immediately before we need them. For a brief intro on what Jupyter notebooks are and how they work, check out [this short tutorial by Jeremy Howard](https://www.kaggle.com/code/jhoward/jupyter-notebook-101).\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "intro0"
        ],
        "id": "_Sr-QGoEJmaE"
      },
      "source": [
        "### 0: Python Review\n",
        "Before we even get started though, let's quickly review a few important classes/datatypes that will come up often when working with Deep Learning models. The first of these is the [`pandas`](https://pandas.pydata.org/docs/index.html) [`DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html).\n",
        "\n",
        "To start, we'll import pandas and create a DataFrame object. But, first, let's create a simple Python [`dictionary`](https://docs.python.org/3/tutorial/datastructures.html#dictionaries):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvtMXCtkJmaE",
        "outputId": "f02582ff-cea7-474c-b960-72f7dafbcf18"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'col_a': [1, 2, 3, 4, 5], 'col_b': ['blue', 'red', 'red', 'purple', 'red']}"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "toy_dict = {'col_a':[1,2,3,4,5], 'col_b':['blue', 'red', 'red', 'purple', 'red']}\n",
        "toy_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aidG_tYhJmaE"
      },
      "source": [
        "Above we created a toy example of a pandas DataFrame. Most often a data frame will be created by reading an input file, but it's also possible to create one manually using a dictionary. We can verify the datatype is `dict` by using the `type()` function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Egf9GJYJmaE",
        "outputId": "95476fbd-9514-459a-f3d4-531b00b204e2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "type(toy_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrXtouW5JmaE"
      },
      "source": [
        "This dictionary can then be used to create a pandas data frame, where each `dict` key is a column name, and each corresponding `dict` value (which is a list) defines the data in that column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "scIQgls9JmaE",
        "outputId": "4c65f9d6-dfd8-441f-9fd2-ccaa264b13eb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   col_a   col_b\n",
              "0      1    blue\n",
              "1      2     red\n",
              "2      3     red\n",
              "3      4  purple\n",
              "4      5     red"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0db791ec-6efa-4bcc-b80d-93598c0e1bc2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>col_a</th>\n",
              "      <th>col_b</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>blue</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>red</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>red</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>purple</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>red</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0db791ec-6efa-4bcc-b80d-93598c0e1bc2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0db791ec-6efa-4bcc-b80d-93598c0e1bc2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0db791ec-6efa-4bcc-b80d-93598c0e1bc2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_5829008d-b928-43cc-8757-a785bf730cac\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('toy_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_5829008d-b928-43cc-8757-a785bf730cac button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('toy_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "toy_df",
              "summary": "{\n  \"name\": \"toy_df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"col_a\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 5,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          2,\n          5,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"col_b\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"blue\",\n          \"red\",\n          \"purple\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "toy_df = pd.DataFrame(toy_dict)\n",
        "toy_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsZfi2xjJmaE"
      },
      "source": [
        "### 1: Loading Dataset\n",
        "Now getting back to our problem... we want to use the Airline Tweets (text) dataset to create a simple model for predicting the tweet sentiment. To do so we'll need to load the dataset into a `pandas` `DataFrame`. If the data is stored locally then we would use `pd.read_csv(\"path/to/file/file.csv\")` to open it. In this case, the data is online at GitHub. Fortunately, the `pandas` module also knows how to open a file from a URL without any additional parameters. So, we can still use the same method but with the URL instead of the local path, as seen below.\n",
        "\n",
        "After loading the data file, we'll check what the dimensions of the data frame are (i.e. number of rows and number of columns - shown together in a single Python [`tuple`](https://docs.python.org/3/tutorial/datastructures.html#tuples-and-sequences))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEJ7Ks1xJmaE",
        "outputId": "81ae5887-ac38-4083-e033-2232abce05fb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "data_URL = 'https://raw.githubusercontent.com/sgeinitz/DSML4220/main/data/airlinetweets.csv'\n",
        "df = pd.read_csv(data_URL)\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKR4USGdJmaF"
      },
      "source": [
        "Now let's look at the first few observations (i.e. tweets) in the data frame using the DataFrame's `head` method. This is first and most basic step we can do in what is called, [Exploratory Data Analysis (EDA)](https://en.wikipedia.org/wiki/Exploratory_data_analysis). We will not dig much into actual EDA in this notebook, but it's important to note that this is a critical step in any data analysis or modeling project. If we were to complete a full EDA though, there are many tools that can help out, such as the [ydata-profiling project](https://github.com/ydataai/ydata-profiling) Python module. Perhaps not surprising, AI tools and LLMs (e.g. ChatGPT) can even perform a simple EDA on a dataset.\n",
        "\n",
        " For now we will simply peek at the first few rows.\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "Cv-0Uy5IJmaF",
        "outputId": "e46a71b1-892c-4da0-e9dc-21963b3b3522"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  sentiment                                               text\n",
              "0  positive  @JetBlue @JayVig I like the inflight snacks! I...\n",
              "1  positive  @VirginAmerica thanks guys! Sweet route over t...\n",
              "2  negative  @USAirways Your exchange/credit policies are w...\n",
              "3  negative  @USAirways but in the meantime I'll be sleepin...\n",
              "4  negative  @VirginAmerica hold times at call center are a...\n",
              "5  negative  @USAirways not moving we are in the tarmac del...\n",
              "6   neutral  @JetBlue What about if I booked it through Orb...\n",
              "7  negative  @united 2nd flight also delayed no pilots! But...\n",
              "8  negative  .@AmericanAir after 50 minutes on hold, and an...\n",
              "9  positive        @JetBlue flight 117. proud to fly Jet Blue!"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5f72f6a8-1837-4e6e-b7e5-e618790a6bc1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>positive</td>\n",
              "      <td>@JetBlue @JayVig I like the inflight snacks! I...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>positive</td>\n",
              "      <td>@VirginAmerica thanks guys! Sweet route over t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>negative</td>\n",
              "      <td>@USAirways Your exchange/credit policies are w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>negative</td>\n",
              "      <td>@USAirways but in the meantime I'll be sleepin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>negative</td>\n",
              "      <td>@VirginAmerica hold times at call center are a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>negative</td>\n",
              "      <td>@USAirways not moving we are in the tarmac del...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>neutral</td>\n",
              "      <td>@JetBlue What about if I booked it through Orb...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>negative</td>\n",
              "      <td>@united 2nd flight also delayed no pilots! But...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>negative</td>\n",
              "      <td>.@AmericanAir after 50 minutes on hold, and an...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>positive</td>\n",
              "      <td>@JetBlue flight 117. proud to fly Jet Blue!</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5f72f6a8-1837-4e6e-b7e5-e618790a6bc1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-5f72f6a8-1837-4e6e-b7e5-e618790a6bc1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-5f72f6a8-1837-4e6e-b7e5-e618790a6bc1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 10000,\n  \"fields\": [\n    {\n      \"column\": \"sentiment\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"positive\",\n          \"negative\",\n          \"neutral\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9901,\n        \"samples\": [\n          \".@USAirways we r rebooked. got conflicting info abt baggage. Y no extra plane batteries on hand? Y no comped admirals club for 9 hr wait?\",\n          \"@united airlines is the absolute worst. They have no idea what they are doing. #neveragain #UnitedAirlines\",\n          \"@united website says my flight is on time. It leaves in 15 min and nobody has boarded yet. #pathetic #needtobehonest\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVnC9EW-JmaF"
      },
      "source": [
        "To be able to see the the full __text__ field/column we need to tell pandas to change its default column width to be displayed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "PukAOkdEJmaF",
        "outputId": "ee60d7ce-ebcc-4305-f185-fb31d10d22ac"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  sentiment  \\\n",
              "0  positive   \n",
              "1  positive   \n",
              "2  negative   \n",
              "3  negative   \n",
              "4  negative   \n",
              "\n",
              "                                                                                                                            text  \n",
              "0                                        @JetBlue @JayVig I like the inflight snacks! I'm flying with you guys on 2/28! #JVMChat  \n",
              "1                                                    @VirginAmerica thanks guys! Sweet route over the Rockies #airplanemodewason  \n",
              "2  @USAirways Your exchange/credit policies are worthless and shadier than the White House. Dissatisfied to the nines right now.  \n",
              "3                                  @USAirways but in the meantime I'll be sleeping on a park bench on dadeland st.  Thanks guys!  \n",
              "4                                                                        @VirginAmerica hold times at call center are a bit much  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-704ca913-2d40-4eef-8f3f-91a8b7444035\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>positive</td>\n",
              "      <td>@JetBlue @JayVig I like the inflight snacks! I'm flying with you guys on 2/28! #JVMChat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>positive</td>\n",
              "      <td>@VirginAmerica thanks guys! Sweet route over the Rockies #airplanemodewason</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>negative</td>\n",
              "      <td>@USAirways Your exchange/credit policies are worthless and shadier than the White House. Dissatisfied to the nines right now.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>negative</td>\n",
              "      <td>@USAirways but in the meantime I'll be sleeping on a park bench on dadeland st.  Thanks guys!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>negative</td>\n",
              "      <td>@VirginAmerica hold times at call center are a bit much</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-704ca913-2d40-4eef-8f3f-91a8b7444035')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-704ca913-2d40-4eef-8f3f-91a8b7444035 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-704ca913-2d40-4eef-8f3f-91a8b7444035');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 10000,\n  \"fields\": [\n    {\n      \"column\": \"sentiment\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"positive\",\n          \"negative\",\n          \"neutral\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9901,\n        \"samples\": [\n          \".@USAirways we r rebooked. got conflicting info abt baggage. Y no extra plane batteries on hand? Y no comped admirals club for 9 hr wait?\",\n          \"@united airlines is the absolute worst. They have no idea what they are doing. #neveragain #UnitedAirlines\",\n          \"@united website says my flight is on time. It leaves in 15 min and nobody has boarded yet. #pathetic #needtobehonest\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "pd.set_option(\"display.max_colwidth\", 240)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TpQgKO6JmaF"
      },
      "source": [
        "\n",
        "We would typically also use the `describe` DataFrame method to see some descriptive statistics for each column, but as we'll see below. It does not provide as much for text data as for numerical data, but this is still useful information. Namely, how many non-empty cells are there in each column, how many unique values in each column, most frequently occuring value, and the frequency of the most frequenly occurring value in each column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "xuLFjtPdJmaG",
        "outputId": "6e570e97-d9f1-4e7b-f9ff-e09240fd0191"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       sentiment                 text\n",
              "count      10000                10000\n",
              "unique         3                 9901\n",
              "top     negative  @AmericanAir thanks\n",
              "freq        6525                    5"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-85bb9fc0-c4da-49c2-85b4-f3b3d006c0ab\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>10000</td>\n",
              "      <td>10000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>3</td>\n",
              "      <td>9901</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>negative</td>\n",
              "      <td>@AmericanAir thanks</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>6525</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-85bb9fc0-c4da-49c2-85b4-f3b3d006c0ab')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-85bb9fc0-c4da-49c2-85b4-f3b3d006c0ab button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-85bb9fc0-c4da-49c2-85b4-f3b3d006c0ab');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"sentiment\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          3,\n          \"6525\",\n          \"10000\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          9901,\n          \"5\",\n          \"10000\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8MFRJrMJmaG"
      },
      "source": [
        "Now let's summarize the observations (i.e. tweets) by their labels (i.e. sentiment). We know there should be three possible values for the labels: positive, neutral, and negative. From above, we already know that _negative_ is the most frequent sentiment, but we can now see how many times the other sentiments (i.e. classes) appear."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "uUMnxgo_JmaG",
        "outputId": "233da8ec-5288-4a17-e9d3-e62b36e82551"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "sentiment\n",
              "negative    0.6525\n",
              "neutral     0.1916\n",
              "positive    0.1559\n",
              "Name: proportion, dtype: float64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>proportion</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sentiment</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>negative</th>\n",
              "      <td>0.6525</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>neutral</th>\n",
              "      <td>0.1916</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>positive</th>\n",
              "      <td>0.1559</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> float64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "df.sentiment.value_counts(normalize=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZOZuBbSJmaH"
      },
      "source": [
        "Let's now see what are the most common words used. Note that this means we need to separate the each tweet into the words. The most basic way to accomplish this is to use Python's string method, __split__. We can first confirm that the _text_ field is in a fact a string by looking at the data frame's data types with __dtypes__."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "_QLhJfXUJmaH",
        "outputId": "846ff5b8-23d9-47ae-c367-e4ee87196d42"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "sentiment    object\n",
              "text         object\n",
              "dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>sentiment</th>\n",
              "      <td>object</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>text</th>\n",
              "      <td>object</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "df.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXaLBaITJmaH"
      },
      "source": [
        "Note that the _text_ column is not a string data type, but is instead the more general __object__ data type (note that everything in Python inherits from the __object__ data type). Because of Python's friendly dynamic behavior, we don't need to worry about this if we were to look at a single tweet. That is, Python will allow us to use the string.split() method when we are using a single tweet:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gt9xL2ZpJmaH",
        "outputId": "ae257bca-827e-40de-8698-2b6f53cbbf02"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['@VirginAmerica',\n",
              " 'thanks',\n",
              " 'guys!',\n",
              " 'Sweet',\n",
              " 'route',\n",
              " 'over',\n",
              " 'the',\n",
              " 'Rockies',\n",
              " '#airplanemodewason']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "df.iloc[1,1].split()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojmu3hJ3JmaH"
      },
      "source": [
        "However, if we try to use split on the entire column we'll run into an issue, as can be seen by the error we encountered here (if you uncomment the line in the following cell)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_evx-ZlgJmaH"
      },
      "outputs": [],
      "source": [
        "# df['text'].split()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SiFBMubJmaH"
      },
      "source": [
        "To avoid this we need to use the [`str`](https://pandas.pydata.org/docs/reference/api/pandas.Series.str.html) attribute for the pandas [`DataSeries`](https://pandas.pydata.org/docs/reference/api/pandas.Series.html) object. This will to allow for string methods to be used. We'll now do this for the first 5 rows of data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "wt6AhzgvJmaH",
        "outputId": "6a5b601f-87c1-458a-d6df-4bf462ea353d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                                             [@JetBlue, @JayVig, I, like, the, inflight, snacks!, I'm, flying, with, you, guys, on, 2/28!, #JVMChat]\n",
              "1                                                               [@VirginAmerica, thanks, guys!, Sweet, route, over, the, Rockies, #airplanemodewason]\n",
              "2    [@USAirways, Your, exchange/credit, policies, are, worthless, and, shadier, than, the, White, House., Dissatisfied, to, the, nines, right, now.]\n",
              "3                                      [@USAirways, but, in, the, meantime, I'll, be, sleeping, on, a, park, bench, on, dadeland, st., Thanks, guys!]\n",
              "4                                                                                  [@VirginAmerica, hold, times, at, call, center, are, a, bit, much]\n",
              "Name: text, dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[@JetBlue, @JayVig, I, like, the, inflight, snacks!, I'm, flying, with, you, guys, on, 2/28!, #JVMChat]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[@VirginAmerica, thanks, guys!, Sweet, route, over, the, Rockies, #airplanemodewason]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[@USAirways, Your, exchange/credit, policies, are, worthless, and, shadier, than, the, White, House., Dissatisfied, to, the, nines, right, now.]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[@USAirways, but, in, the, meantime, I'll, be, sleeping, on, a, park, bench, on, dadeland, st., Thanks, guys!]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[@VirginAmerica, hold, times, at, call, center, are, a, bit, much]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "df['text'][:5].str.split()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Zn_lVSjJmaI"
      },
      "source": [
        "Let's now add a new column that contains the list of tokens (i.e. words) for each tweet. Note that the [`split()`](https://pandas.pydata.org/docs/reference/api/pandas.Series.str.split.html#pandas-series-str-split) method splits a string by the SPACE character by default. This looks alright for now, but we'll see how to improve this later on. Now, however, that we will also convert everything to lowercase before splitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        },
        "id": "p96zZLN2JmaI",
        "outputId": "a8249084-e991-43ff-fb67-a5951c4629a8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  sentiment  \\\n",
              "0  positive   \n",
              "1  positive   \n",
              "2  negative   \n",
              "3  negative   \n",
              "4  negative   \n",
              "\n",
              "                                                                                                                            text  \\\n",
              "0                                        @JetBlue @JayVig I like the inflight snacks! I'm flying with you guys on 2/28! #JVMChat   \n",
              "1                                                    @VirginAmerica thanks guys! Sweet route over the Rockies #airplanemodewason   \n",
              "2  @USAirways Your exchange/credit policies are worthless and shadier than the White House. Dissatisfied to the nines right now.   \n",
              "3                                  @USAirways but in the meantime I'll be sleeping on a park bench on dadeland st.  Thanks guys!   \n",
              "4                                                                        @VirginAmerica hold times at call center are a bit much   \n",
              "\n",
              "                                                                                                                                             tokens  \n",
              "0                                           [@jetblue, @jayvig, i, like, the, inflight, snacks!, i'm, flying, with, you, guys, on, 2/28!, #jvmchat]  \n",
              "1                                                             [@virginamerica, thanks, guys!, sweet, route, over, the, rockies, #airplanemodewason]  \n",
              "2  [@usairways, your, exchange/credit, policies, are, worthless, and, shadier, than, the, white, house., dissatisfied, to, the, nines, right, now.]  \n",
              "3                                    [@usairways, but, in, the, meantime, i'll, be, sleeping, on, a, park, bench, on, dadeland, st., thanks, guys!]  \n",
              "4                                                                                [@virginamerica, hold, times, at, call, center, are, a, bit, much]  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-25bcf1bb-2564-4ea8-9d23-1e34e2061d5b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>text</th>\n",
              "      <th>tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>positive</td>\n",
              "      <td>@JetBlue @JayVig I like the inflight snacks! I'm flying with you guys on 2/28! #JVMChat</td>\n",
              "      <td>[@jetblue, @jayvig, i, like, the, inflight, snacks!, i'm, flying, with, you, guys, on, 2/28!, #jvmchat]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>positive</td>\n",
              "      <td>@VirginAmerica thanks guys! Sweet route over the Rockies #airplanemodewason</td>\n",
              "      <td>[@virginamerica, thanks, guys!, sweet, route, over, the, rockies, #airplanemodewason]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>negative</td>\n",
              "      <td>@USAirways Your exchange/credit policies are worthless and shadier than the White House. Dissatisfied to the nines right now.</td>\n",
              "      <td>[@usairways, your, exchange/credit, policies, are, worthless, and, shadier, than, the, white, house., dissatisfied, to, the, nines, right, now.]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>negative</td>\n",
              "      <td>@USAirways but in the meantime I'll be sleeping on a park bench on dadeland st.  Thanks guys!</td>\n",
              "      <td>[@usairways, but, in, the, meantime, i'll, be, sleeping, on, a, park, bench, on, dadeland, st., thanks, guys!]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>negative</td>\n",
              "      <td>@VirginAmerica hold times at call center are a bit much</td>\n",
              "      <td>[@virginamerica, hold, times, at, call, center, are, a, bit, much]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-25bcf1bb-2564-4ea8-9d23-1e34e2061d5b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-25bcf1bb-2564-4ea8-9d23-1e34e2061d5b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-25bcf1bb-2564-4ea8-9d23-1e34e2061d5b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 10000,\n  \"fields\": [\n    {\n      \"column\": \"sentiment\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"positive\",\n          \"negative\",\n          \"neutral\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9901,\n        \"samples\": [\n          \".@USAirways we r rebooked. got conflicting info abt baggage. Y no extra plane batteries on hand? Y no comped admirals club for 9 hr wait?\",\n          \"@united airlines is the absolute worst. They have no idea what they are doing. #neveragain #UnitedAirlines\",\n          \"@united website says my flight is on time. It leaves in 15 min and nobody has boarded yet. #pathetic #needtobehonest\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tokens\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "df['tokens'] = df['text'].str.lower().str.split()\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0HvtpPoJmaI"
      },
      "source": [
        "### 2: Develop Simple Model\n",
        "\n",
        "Let's now get into our naive model approach we discussed earlier. The idea was to count how often each word (i.e. token) appears in all of the positive tweets, in all of the negative tweets, and in all of the neutral tweets. Before that though, let's simply count how often each word occurs across all of the tweets. We'll use a `dict` datatype for this,  each __key__ is a word and the key's __value__ will be the number of times that word appears."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbRc2o9xJmaI",
        "outputId": "4261de0c-d68d-4183-b8d2-1338692bc890"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20975"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "vocab = dict()\n",
        "for tweet_tokens in df['tokens']:\n",
        "    for token in tweet_tokens:\n",
        "        if token not in vocab:\n",
        "            vocab[token] = 1\n",
        "        else:\n",
        "            vocab[token] += 1\n",
        "\n",
        "len(vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IwLnyy0JmaI"
      },
      "source": [
        "Let's sort these by the frequency with which each word (i.e. token) appears and then look at the top 20 or so."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6iAs-OTJmaI",
        "outputId": "734c068a-f39b-4305-f0c9-24ef6afcac92"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('to', 5881),\n",
              " ('the', 4117),\n",
              " ('i', 3625),\n",
              " ('a', 3149),\n",
              " ('for', 2770),\n",
              " ('on', 2596),\n",
              " ('and', 2587),\n",
              " ('@united', 2569),\n",
              " ('you', 2510),\n",
              " ('my', 2282),\n",
              " ('flight', 2243),\n",
              " ('@usairways', 2061),\n",
              " ('@americanair', 1989),\n",
              " ('is', 1921),\n",
              " ('in', 1707),\n",
              " ('@southwestair', 1619),\n",
              " ('of', 1490),\n",
              " ('@jetblue', 1352),\n",
              " ('your', 1208),\n",
              " ('have', 1138),\n",
              " ('was', 1100),\n",
              " ('with', 1055),\n",
              " ('me', 1053),\n",
              " ('it', 1051),\n",
              " ('not', 1047)]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "vocab_sorted = dict(sorted(vocab.items(), key=lambda item: item[1], reverse=True))\n",
        "list(vocab_sorted.items())[:25]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dySkEqpJmaI"
      },
      "source": [
        "Not surprising that words such as, \"_to_\", \"_the_\", \"_i_\", etc. are the most frequently appearing. It seems that most of these will also not convey much information in terms of sentiment. To prevent such uninformative words from influencing the task at hand, most NLP libraries/tasks provide an easy way to remove stop words.\n",
        "\n",
        "Before utilizing to a Python module designed to work with text data, let's first try continuing with our simple manual approach. Let's see if we can see some difference between in the word (i.e. token) frequencies when we separate the data frame into positive, neutral, and negative."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "UwKrGJn1JmaI"
      },
      "outputs": [],
      "source": [
        "df_pos = df[df['sentiment'] == 'positive']\n",
        "df_neg = df[df['sentiment'] == 'negative']\n",
        "df_neu = df[df['sentiment'] == 'neutral']\n",
        "\n",
        "def create_vocab_list(tokens_column):\n",
        "    vocab = dict()\n",
        "    for tweet_tokens in tokens_column:\n",
        "        for token in tweet_tokens:\n",
        "            if token not in vocab:\n",
        "                vocab[token] = 1\n",
        "            else:\n",
        "                vocab[token] += 1\n",
        "    return vocab\n",
        "\n",
        "vocab_pos = dict(sorted(create_vocab_list(df_pos['tokens']).items(), key=lambda item: item[1], reverse=True))\n",
        "vocab_neg = dict(sorted(create_vocab_list(df_neg['tokens']).items(), key=lambda item: item[1], reverse=True))\n",
        "vocab_neu = dict(sorted(create_vocab_list(df_neu['tokens']).items(), key=lambda item: item[1], reverse=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9-X6ygjJmaI"
      },
      "source": [
        "Let's now output the most frequently occuring words for each sentiment class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmuCD41TJmaI",
        "outputId": "c0e862b5-418a-417f-f345-62a002c4b067"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('the', 646),\n",
              " ('to', 601),\n",
              " ('for', 460),\n",
              " ('you', 421),\n",
              " ('i', 383),\n",
              " ('@southwestair', 381),\n",
              " ('a', 350),\n",
              " ('@jetblue', 340),\n",
              " ('@united', 330),\n",
              " ('thank', 320),\n",
              " ('and', 284),\n",
              " ('thanks', 263),\n",
              " ('my', 245),\n",
              " ('@americanair', 229),\n",
              " ('in', 216)]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "list(vocab_pos.items())[:15]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGFl6IM1JmaI",
        "outputId": "5decee8f-571a-42bf-b581-8067c7919985"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('to', 4237),\n",
              " ('the', 2898),\n",
              " ('i', 2519),\n",
              " ('a', 2296),\n",
              " ('and', 2049),\n",
              " ('on', 1966),\n",
              " ('for', 1930),\n",
              " ('@united', 1828),\n",
              " ('my', 1699),\n",
              " ('flight', 1697),\n",
              " ('you', 1694),\n",
              " ('@usairways', 1671),\n",
              " ('is', 1483),\n",
              " ('@americanair', 1467),\n",
              " ('in', 1219)]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "list(vocab_neg.items())[:15]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avEasoarJmaI",
        "outputId": "bbc9b155-f6f8-4262-e70b-79698a2f8d9c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('to', 1043),\n",
              " ('i', 723),\n",
              " ('the', 573),\n",
              " ('a', 503),\n",
              " ('on', 415),\n",
              " ('@southwestair', 414),\n",
              " ('@united', 411),\n",
              " ('you', 395),\n",
              " ('for', 380),\n",
              " ('@jetblue', 365),\n",
              " ('my', 338),\n",
              " ('flight', 337),\n",
              " ('@americanair', 293),\n",
              " ('is', 291),\n",
              " ('can', 278)]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "list(vocab_neu.items())[:15]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VPdNiq5JmaI"
      },
      "source": [
        "The output above confirms that many of the overall most frequently occuring words are not correlated in anyway with the sentiment of the tweet since, for example, \"_to_\", is the first or second frequently occurring for each separate group. In other words, there still a lot of stop words in the vocabularies for positive, negative and neutral tweets. Let's try removing the tokens from each of these vocabularies, if the token is also in the top, say, $100$, tokens overall.\n",
        "<a id=\"topn2remove\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "Gt9gafLVJmaJ"
      },
      "outputs": [],
      "source": [
        "top_n_to_remove = 100\n",
        "for item in list(vocab_sorted.items())[:top_n_to_remove]:\n",
        "    #print(f\" removing token: {item[0]:15} (w/ freq = {item[1]:5}) from vocabs\")\n",
        "    if item[0] in vocab_pos:\n",
        "        del vocab_pos[item[0]]\n",
        "    if item[0] in vocab_neg:\n",
        "        del vocab_neg[item[0]]\n",
        "    if item[0] in vocab_neu:\n",
        "        del vocab_neu[item[0]]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Maxno494JmaJ",
        "outputId": "f760de5d-1742-4613-dbf3-ee5f649f3f06"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(':-)', 16),\n",
              " ('kudos', 13),\n",
              " ('great!', 13),\n",
              " ('rock!', 13),\n",
              " ('awesome!', 13),\n",
              " ('wonderful', 12),\n",
              " ('excited', 11),\n",
              " ('you!!', 11),\n",
              " ('much.', 10),\n",
              " ('keeping', 10),\n",
              " ('much!', 10),\n",
              " ('passbook', 10),\n",
              " ('favorite', 9),\n",
              " ('@fortunemagazine', 9),\n",
              " ('you!!!', 9)]"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ],
      "source": [
        "list(vocab_pos.items())[:15]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qb53sdZIJmaJ",
        "outputId": "7334c97d-d47c-4620-ed32-e03433db0090"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(\"weren't\", 17),\n",
              " ('min.', 17),\n",
              " ('suck', 16),\n",
              " ('cut', 16),\n",
              " ('switched', 16),\n",
              " ('screw', 16),\n",
              " ('worst.', 16),\n",
              " ('fucking', 16),\n",
              " ('hours,', 16),\n",
              " ('ruined', 16),\n",
              " ('load', 16),\n",
              " ('lines', 16),\n",
              " ('failed', 15),\n",
              " ('90', 15),\n",
              " ('refuse', 15)]"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ],
      "source": [
        "list(vocab_neg.items())[:15]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bw4wHEjhJmaJ",
        "outputId": "9d974e28-92bb-4449-c4dd-ce5db8245a26"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('street', 13),\n",
              " ('#avgeek', 12),\n",
              " ('journal', 11),\n",
              " ('hi,', 11),\n",
              " ('battles', 10),\n",
              " ('appease', 10),\n",
              " ('daily', 10),\n",
              " ('nashville', 9),\n",
              " ('feb', 9),\n",
              " ('routes', 9),\n",
              " ('departing', 9),\n",
              " ('waterbury', 8),\n",
              " ('republican', 8),\n",
              " ('march', 8),\n",
              " ('hi!', 8)]"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ],
      "source": [
        "list(vocab_neu.items())[:15]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsOfBobZJmaJ"
      },
      "source": [
        "That looks a little better! Now, let's try classifying the tweets by looking at one and counting how many tokens it has from the top k tokens in the vocab_pos, vocab_neg, and vocab_neutral sets. Whichever vocab it has the greatest number of tokens from, let's classify it as that.\n",
        "\n",
        "To accomplish this let's first create a vocabulary for each of the possible label values. Note that below we are including all the tokens for each label but we could easily include just the top k positive tokens, top k negative, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "McdAn_jrJmaJ"
      },
      "outputs": [],
      "source": [
        "classifier_tokens = {\"positive\": list(vocab_pos.keys())[:], \"negative\": list(vocab_neg.keys())[:], \"neutral\": list(vocab_neu.keys())[:]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jrIkXOzJmaJ"
      },
      "source": [
        "Let's count exactly how many tokens will be considered for each class. It's not surprising that there are the most negative tokens. That's because most of the tweets are negative, so there are more unique tokens among the negative tweets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILV5py1WJmaJ",
        "outputId": "6d32f658-b52d-4609-a14c-f09d9d2ee2de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "positive tokens: 4198       \n",
            "negative tokens: 14338       \n",
            "neutral tokens: 5482\n"
          ]
        }
      ],
      "source": [
        "print(f\"positive tokens: {len(classifier_tokens['positive'])} \\\n",
        "      \\nnegative tokens: {len(classifier_tokens['negative'])} \\\n",
        "      \\nneutral tokens: {len(classifier_tokens['neutral'])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QD0U8UInJmaJ"
      },
      "source": [
        "Let's see what the classifier_tokens dictionary looks like\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfezSSksJmaJ",
        "outputId": "652d15ba-16af-4c00-a34b-6d53df45a2e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "top 10 tokens for positive tweets: \n",
            "    [':-)', 'kudos', 'great!', 'rock!', 'awesome!', 'wonderful', 'excited', 'you!!', 'much.', 'keeping', 'much!', 'passbook', 'favorite', '@fortunemagazine', 'you!!!', '', 'home!', 'today!', 'prompt', 'amazing.']\n",
            "top 10 tokens for negative tweets: \n",
            "    [\"weren't\", 'min.', 'suck', 'cut', 'switched', 'screw', 'worst.', 'fucking', 'hours,', 'ruined', 'load', 'lines', 'failed', '90', 'refuse', 'screwed', 'fail', 'plane,', 'why?', 'total']\n",
            "top 10 tokens for neutral tweets: \n",
            "    ['street', '#avgeek', 'journal', 'hi,', 'battles', 'appease', 'daily', 'nashville', 'feb', 'routes', 'departing', 'waterbury', 'republican', 'march', 'hi!', 'tomorrow?', 'fl', '#flyingitforward', 'domestic', 'deals']\n"
          ]
        }
      ],
      "source": [
        "# let's see the top 20 tokens for each sentiment\n",
        "for sentiment in classifier_tokens:\n",
        "    print(f\"top 10 tokens for {sentiment} tweets: \\n    {classifier_tokens[sentiment][:20]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8YwQn_gJmaJ"
      },
      "source": [
        "Okay, we're almost done. Let's now define the so-called model by implementing a `predict` function. There are many variations of this that could be done, but this is what we have."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "NcEgyE9hJmaJ"
      },
      "outputs": [],
      "source": [
        "def predict_tweet_sentiment(tweet_tokens, verbose=False):\n",
        "    if verbose:\n",
        "        print(f\"tweet: {' '.join(tweet_tokens)}\")\n",
        "    pos = 0\n",
        "    neg = 0\n",
        "    neu = 0\n",
        "    for tok in tweet_tokens:\n",
        "        if tok in classifier_tokens['positive']:\n",
        "            pos += 1\n",
        "        elif tok in classifier_tokens['negative']:\n",
        "            neg += 1\n",
        "        elif tok in classifier_tokens['neutral']:\n",
        "            neu += 1\n",
        "    if verbose:\n",
        "        print(f\"counts: \\n  postive tokens: {pos}\\n  negative tokens: {neg}\\n  neutral tokens: {neu}\")\n",
        "    if pos > neg and pos > neu:\n",
        "        prediction = \"positive\"\n",
        "    elif neu > pos and neu > neg:\n",
        "        prediction = \"neutral\"\n",
        "    else:\n",
        "        prediction = \"negative\"\n",
        "    if verbose:\n",
        "        print(f\"prediction: {prediction}\")\n",
        "    return prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmHGghHuJmaK"
      },
      "source": [
        "Let's see what happens when we try to classify one single tweet using our `predict` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1TBCsR1JmaK",
        "outputId": "50c17d1e-09dc-4a71-cccb-f0b06384fb5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentiment                                                                                                                                                                          negative\n",
            "text                                             @AmericanAir thx for responding. I cant watch 2 mins of this film w/out it cutting in and out 4 prolonged prds of time. beyond frustrating\n",
            "tokens                 [@americanair, thx, for, responding., i, cant, watch, 2, mins, of, this, film, w/out, it, cutting, in, and, out, 4, prolonged, prds, of, time., beyond, frustrating]\n",
            "predicted_sentiment                                                                                                                                                                negative\n",
            "Name: 100, dtype: object\n"
          ]
        }
      ],
      "source": [
        "i = 100\n",
        "print(df.iloc[i,:])\n",
        "tweet2classify = df.iloc[i,:]['tokens']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "id": "c2sHtXieJmaK",
        "outputId": "838f6b3f-0b73-457a-e553-b906f321f3b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tweet: @americanair thx for responding. i cant watch 2 mins of this film w/out it cutting in and out 4 prolonged prds of time. beyond frustrating\n",
            "counts: \n",
            "  postive tokens: 0\n",
            "  negative tokens: 6\n",
            "  neutral tokens: 0\n",
            "prediction: negative\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'negative'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 114
        }
      ],
      "source": [
        "predict_tweet_sentiment(tweet2classify, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQSQ0D73JmaK"
      },
      "source": [
        "### 3: Evaluate Simple Model\n",
        "\n",
        "Let's now make predictions for all of the tweets in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "waYer7abJmaK"
      },
      "outputs": [],
      "source": [
        "df['predicted_sentiment'] = df['tokens'].apply(lambda x: predict_tweet_sentiment(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LS6Q02VBJmaK"
      },
      "source": [
        "Let's look at a few of the predictions as compared to the actual (i.e. true) labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "rlu1gn4EJmaK",
        "outputId": "def8637f-b66d-44ea-e320-2ff756bd6983"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  sentiment predicted_sentiment  \\\n",
              "0  positive            positive   \n",
              "1  positive            positive   \n",
              "2  negative            negative   \n",
              "3  negative            negative   \n",
              "4  negative            positive   \n",
              "5  negative            negative   \n",
              "6   neutral             neutral   \n",
              "7  negative            negative   \n",
              "8  negative            negative   \n",
              "9  positive            positive   \n",
              "\n",
              "                                                                                                                                       text  \n",
              "0                                                   @JetBlue @JayVig I like the inflight snacks! I'm flying with you guys on 2/28! #JVMChat  \n",
              "1                                                               @VirginAmerica thanks guys! Sweet route over the Rockies #airplanemodewason  \n",
              "2             @USAirways Your exchange/credit policies are worthless and shadier than the White House. Dissatisfied to the nines right now.  \n",
              "3                                             @USAirways but in the meantime I'll be sleeping on a park bench on dadeland st.  Thanks guys!  \n",
              "4                                                                                   @VirginAmerica hold times at call center are a bit much  \n",
              "5                                          @USAirways not moving we are in the tarmac delayed for some unknown reason. I'll keep you posted  \n",
              "6                                       @JetBlue What about if I booked it through Orbitz? My email is correct, but there's a middle party.  \n",
              "7                                       @united 2nd flight also delayed no pilots! But they boarded is so we can just sit here! #scheduling  \n",
              "8  .@AmericanAir after 50 minutes on hold, and another 30 minutes on the call yes. Going to be pushing it to get to the airport on time now  \n",
              "9                                                                                               @JetBlue flight 117. proud to fly Jet Blue!  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0f1a1bca-4a9f-41df-a21d-94f87fde4cca\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>predicted_sentiment</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "      <td>@JetBlue @JayVig I like the inflight snacks! I'm flying with you guys on 2/28! #JVMChat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "      <td>@VirginAmerica thanks guys! Sweet route over the Rockies #airplanemodewason</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>negative</td>\n",
              "      <td>negative</td>\n",
              "      <td>@USAirways Your exchange/credit policies are worthless and shadier than the White House. Dissatisfied to the nines right now.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>negative</td>\n",
              "      <td>negative</td>\n",
              "      <td>@USAirways but in the meantime I'll be sleeping on a park bench on dadeland st.  Thanks guys!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>negative</td>\n",
              "      <td>positive</td>\n",
              "      <td>@VirginAmerica hold times at call center are a bit much</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>negative</td>\n",
              "      <td>negative</td>\n",
              "      <td>@USAirways not moving we are in the tarmac delayed for some unknown reason. I'll keep you posted</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>neutral</td>\n",
              "      <td>neutral</td>\n",
              "      <td>@JetBlue What about if I booked it through Orbitz? My email is correct, but there's a middle party.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>negative</td>\n",
              "      <td>negative</td>\n",
              "      <td>@united 2nd flight also delayed no pilots! But they boarded is so we can just sit here! #scheduling</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>negative</td>\n",
              "      <td>negative</td>\n",
              "      <td>.@AmericanAir after 50 minutes on hold, and another 30 minutes on the call yes. Going to be pushing it to get to the airport on time now</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "      <td>@JetBlue flight 117. proud to fly Jet Blue!</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0f1a1bca-4a9f-41df-a21d-94f87fde4cca')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0f1a1bca-4a9f-41df-a21d-94f87fde4cca button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0f1a1bca-4a9f-41df-a21d-94f87fde4cca');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df[['sentiment','predicted_sentiment','text']]\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"sentiment\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"positive\",\n          \"negative\",\n          \"neutral\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"predicted_sentiment\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"positive\",\n          \"negative\",\n          \"neutral\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \".@AmericanAir after 50 minutes on hold, and another 30 minutes on the call yes. Going to be pushing it to get to the airport on time now\",\n          \"@VirginAmerica thanks guys! Sweet route over the Rockies #airplanemodewason\",\n          \"@USAirways not moving we are in the tarmac delayed for some unknown reason. I'll keep you posted\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 116
        }
      ],
      "source": [
        "df[['sentiment','predicted_sentiment','text']].head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "5fumR9gXJmaK",
        "outputId": "4a3b9e9b-51f3-4175-b740-df345de965fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model accuracy: 81.83%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
        "confusion_matrix(df['sentiment'], df['predicted_sentiment'])\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix(df['sentiment'], df['predicted_sentiment']), display_labels=['negative', 'neutral', 'positive'])\n",
        "disp.plot()\n",
        "\n",
        "mod_accuracy = accuracy_score(df['sentiment'], df['predicted_sentiment'])\n",
        "print(f\"model accuracy: {mod_accuracy*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1KTOGJ5JmaK"
      },
      "source": [
        "***\n",
        "#### Q1\n",
        "Go back to where we declared `top_n_to_remove` ([here](#topn2remove)). Try changing the value to see if you can improve the accuracy of our model. No need to justify or explain the value you use, just try experimenting with considerably smaller or larger values and see if you can improve the accuracy.\n",
        "\n",
        "What is the model accuracy that you get? Were you able to beat ~66% (the accuracy if every tweet were predicted to be negative)?\n",
        "\n",
        "`<input your answer here>`\n",
        "\n",
        "Declaring with top_n_to_remove = 1000 vs. 100 increased the accuracy to 81.83%, which is dramatically more than the ~30% accuracy of top_n_to_remove = 100, with all the false positive reports.\n",
        "\n",
        "***\n",
        "\n",
        "#### Q2\n",
        "Does the model accuracy we found above give you a good idea of how this model will perform on future tweets? If not, then what could you do to get a better idea of how it will perform on future tweets?\n",
        "\n",
        "`<input your answer here>`\n",
        "\n",
        "I don't think it will give us a good idea of how it will perform on future tweets based on this alone. It seems to be heavily biased towards identifying negative tweets because of the tokens identified that signify \"negative\" tweets. Several of the selected tokens could very well appear in positive tweets. Trying to refine how the words are identified in negative tweets could help this.\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V44w4I_1JmaK"
      },
      "source": [
        "### 4: Tokenization via Stemming\n",
        "\n",
        "Our tokenization approach above was very basic. All we did was split the text on spaces. This resulted in there being separate tokens for strings like \"_delay_\" and \"_delayed_\". Let's now try using a better approach to tokenization known as [__stemming__](https://en.wikipedia.org/wiki/Stemming).\n",
        "\n",
        "Recall that stemming is an approach in which words are truncated to attempt to find the same root or base word. We'll use the natural language toolkit ([__nltk__](https://www.nltk.org/index.html)) for this and try it on an example below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C18NCqRqJmaL"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "words = [\"delay\", \"delayed\", \"delays\"]\n",
        "#words = [\"run\", \"running\", \"runs\", \"ran\"]\n",
        "#words = [\"fly\", \"flying\", \"flies\", \"flew\"]\n",
        "stemmer = LancasterStemmer()\n",
        "for word in words:\n",
        "    print(f\"{word} -> {stemmer.stem(word)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nh6jtw5mJmaL"
      },
      "source": [
        "Even though stemming is very simple, it is quick and often gives the desired results (although try it with the other set of words above and see what you get).\n",
        "\n",
        "Before applying the stemming step, we also want to take care of the extra characters or symbols, such as periods, exclamation marks, while keeping the \"@\" and \"#\" symbols in place (since these are often used in tweets). Fortunately, nltk also has a Twitter-specific method to remove these symbols, called the __TweetTokenizer__.  We'll also load the data again to ensure we are starting from the beginning, and remove stopwords."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2-0wXpUJmaL"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(data_URL)\n",
        "print(f\"df.shape: {df.shape}\")\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q0PhR8aiJmaL"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "tk = TweetTokenizer()\n",
        "df['tokens_raw'] = df['text'].apply(lambda x: tk.tokenize(x.lower()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5H5mMqrJmaL"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "stops = set(stopwords.words('english'))\n",
        "chars2remove = set(['.','!','/', '?', ','])\n",
        "df['tokens_raw'] = df['tokens_raw'].apply(lambda x: [w for w in x if w not in stops])\n",
        "df['tokens_raw'] = df['tokens_raw'].apply(lambda x: [w for w in x if w not in chars2remove])\n",
        "df['tokens_raw'] = df['tokens_raw'].apply(lambda x: [w for w in x if not re.match('^#', w)]) # remove hashtags\n",
        "df['tokens_raw'] = df['tokens_raw'].apply(lambda x: [w for w in x if not re.match('^http', w)]) # remove web links\n",
        "df['tokens_raw'] = df['tokens_raw'].apply(lambda x: [w for w in x if not re.match('^@', w)]) # remove web links\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1nYAtv1JmaL"
      },
      "source": [
        "As you can see above, that first step of tokenization is helpful, but did not include stemmming (as you can tell from words such as \"_fly_ ___ing___\", \"_thank_ ___s___\", and \"_sleep_ ___ing___\".  So let's apply the stemming now and see what the results look like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHvbGNEUJmaL"
      },
      "outputs": [],
      "source": [
        "df['tokens'] = df['tokens_raw'].apply(lambda x: [stemmer.stem(w) for w in x])\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u57yaa4TJmaL"
      },
      "source": [
        "Next, let's remove common stop words (e.g. \"_the_\", \"_in_\", etc.). In this next cell we will also remove some characters/punctuation, as well as hashtag tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdiedEznJmaL"
      },
      "source": [
        "Let's now recreate the same simple model that we used earlier, but now with the cleaned up tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kz8IMo_1JmaL"
      },
      "outputs": [],
      "source": [
        "df_pos = df[df['sentiment'] == 'positive']\n",
        "df_neg = df[df['sentiment'] == 'negative']\n",
        "df_neu = df[df['sentiment'] == 'neutral']\n",
        "\n",
        "def create_vocab_list(tokens_column):\n",
        "    vocab = dict()\n",
        "    for tweet_tokens in tokens_column:\n",
        "        for token in tweet_tokens:\n",
        "            if token not in vocab:\n",
        "                vocab[token] = 1\n",
        "            else:\n",
        "                vocab[token] += 1\n",
        "    return vocab\n",
        "\n",
        "vocab_pos = dict(sorted(create_vocab_list(df_pos['tokens']).items(), key=lambda item: item[1], reverse=True))\n",
        "vocab_neg = dict(sorted(create_vocab_list(df_neg['tokens']).items(), key=lambda item: item[1], reverse=True))\n",
        "vocab_neu = dict(sorted(create_vocab_list(df_neu['tokens']).items(), key=lambda item: item[1], reverse=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sOksFBIOJmaL"
      },
      "outputs": [],
      "source": [
        "list(vocab_pos.items())[:10]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqgXqDKUJmaL"
      },
      "outputs": [],
      "source": [
        "list(vocab_neg.items())[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cx4gB9CTJmaL"
      },
      "outputs": [],
      "source": [
        "list(vocab_neu.items())[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hs7tWJiyJmaL"
      },
      "outputs": [],
      "source": [
        "classifier_tokens = {\"positive\": list(vocab_pos.keys())[:], \"negative\": list(vocab_neg.keys())[:], \"neutral\": list(vocab_neu.keys())[:]}\n",
        "print(f\"positive tokens: {len(classifier_tokens['positive'])} \\\n",
        "      \\nnegative tokens: {len(classifier_tokens['negative'])} \\\n",
        "      \\nneutral tokens: {len(classifier_tokens['neutral'])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiCkCWpZJmaL"
      },
      "source": [
        "***\n",
        "#### Q3\n",
        "In [Q2](#q2) you were asked about the model accuracy we found earlier, and whether we could expect it to be representative of the accuracy we'd see on future, unseen, tweets. Hopefully you realized that without a proper train-test split, we won't have a good idea. Would right now (after we've created the classifier_tokens), be a good time to create a train-test split? Why or why not?\n",
        "\n",
        "`<input your answer here>`\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6ORrkcgJmaL"
      },
      "outputs": [],
      "source": [
        "i = 100\n",
        "print(df.iloc[i,:])\n",
        "tweet2classify = df.iloc[i,:]['tokens']\n",
        "predict_tweet_sentiment(tweet2classify, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-jPF1IlJmaL"
      },
      "source": [
        "Suprisingly, the tweet above was misclassified this time, but it was correctly classified back before we even used stemming. Let's see now how it performs on all $10k$ tweets though."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73Stms09JmaM"
      },
      "outputs": [],
      "source": [
        "df['predicted_sentiment'] = df['tokens'].apply(lambda x: predict_tweet_sentiment(x))\n",
        "df[['sentiment','predicted_sentiment','text', 'tokens']].head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eptwZLfZJmaM"
      },
      "source": [
        "Let's see if that misclassification was just a fluke, and whether this model with stemming has higher accuracy than the earlier model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWgAWgOQJmaM"
      },
      "outputs": [],
      "source": [
        "confusion_matrix(df['sentiment'], df['predicted_sentiment'])\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix(df['sentiment'], df['predicted_sentiment']), display_labels=['negative', 'neutral', 'positive'])\n",
        "disp.plot()\n",
        "mod_accuracy = accuracy_score(df['sentiment'], df['predicted_sentiment'])\n",
        "print(f\"model accuracy: {mod_accuracy*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGKKiNsOJmaM"
      },
      "source": [
        "These results are actually considerably worse than what we saw before. It's not because of the stemming though, it's because even after removing stop words, there are still many words that frequently appear regardless of the sentiment. In the first, simple approach, we removed frequently occurring words like these, but this time we didn't. If we did, then tokens such as \"__flight__\", or \"__hav__\", would also be removed, and we'd see much higher accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQ-pwwPGJmaM"
      },
      "source": [
        "### 5: Tokenization via Lemmatization\n",
        "\n",
        "Let's now try using Lemmatization. We'll again start with a clean version of the data and use the TweetTokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tFHGqjuJmaM"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(data_URL)\n",
        "df['tokens_raw'] = df['text'].apply(lambda x: tk.tokenize(x.lower()))\n",
        "print(f\"df.shape: {df.shape}\")\n",
        "df['tokens_raw'] = df['tokens_raw'].apply(lambda x: [w for w in x if w not in stops])\n",
        "df['tokens_raw'] = df['tokens_raw'].apply(lambda x: [w for w in x if w not in chars2remove])\n",
        "df['tokens_raw'] = df['tokens_raw'].apply(lambda x: [w for w in x if not re.match('^#', w)]) # remove hashtags\n",
        "df['tokens_raw'] = df['tokens_raw'].apply(lambda x: [w for w in x if not re.match('^http', w)]) # remove web links\n",
        "df['tokens_raw'] = df['tokens_raw'].apply(lambda x: [w for w in x if not re.match('^@', w)]) # remove web links\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcZ1qjBHJmaM"
      },
      "source": [
        "\n",
        "Since Lemmatization can convert words such as \"_ran_\" into \"_run_\", it requires knowledge of the English language. Because of that, you may or may not need to first download a dictionary of sorts. To do this, uncomment the following code cell and run it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xaRqNg0SJmaM"
      },
      "outputs": [],
      "source": [
        " #import nltk\n",
        " #nltk.download('wordnet')\n",
        " #nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwZdhfl4JmaM"
      },
      "source": [
        "If the dictionary is available, then the following code will run correctly. If the following cell encounters an error, then uncomment the above cell and run it.\n",
        "\n",
        "Assuming the dictionary is available, then let's go ahead and carry out the Lemmatization. Note that there are much better ways to do this but that we want to use a simple lemmatizer. For example, some lemmatizers also utilize a model internally to predict the part-of-speech for each word, since whether the word is a noun, adjective, verb, etc. will affect how lemmatization is done. Since we want to keep things simple here, and focus only on the lemmatization step, we'll assume every word is the same part of speech. Note that this is not by any means ideal (try to identify the incorrectly lemmatized token in the five tweets printed out below). In practice we would utilize a 'smarter' lemmatizer ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AvVfNt8tJmaM"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "df['tokens'] = df['tokens_raw'].apply(lambda x: [lemmatizer.lemmatize(w, pos=\"v\") for w in x])\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dpqamnwJmaM"
      },
      "source": [
        "Let's now recreate the same simple model that we used earlier, but now with the cleaned up tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uw7k9y3GJmaN"
      },
      "outputs": [],
      "source": [
        "df_pos = df[df['sentiment'] == 'positive']\n",
        "df_neg = df[df['sentiment'] == 'negative']\n",
        "df_neu = df[df['sentiment'] == 'neutral']\n",
        "\n",
        "def create_vocab_list(tokens_column):\n",
        "    vocab = dict()\n",
        "    for tweet_tokens in tokens_column:\n",
        "        for token in tweet_tokens:\n",
        "            if token not in vocab:\n",
        "                vocab[token] = 1\n",
        "            else:\n",
        "                vocab[token] += 1\n",
        "    return vocab\n",
        "\n",
        "vocab_pos = dict(sorted(create_vocab_list(df_pos['tokens']).items(), key=lambda item: item[1], reverse=True))\n",
        "vocab_neg = dict(sorted(create_vocab_list(df_neg['tokens']).items(), key=lambda item: item[1], reverse=True))\n",
        "vocab_neu = dict(sorted(create_vocab_list(df_neu['tokens']).items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "classifier_tokens = {\"positive\": list(vocab_pos.keys())[:], \"negative\": list(vocab_neg.keys())[:], \"neutral\": list(vocab_neu.keys())[:]}\n",
        "print(f\"positive tokens: {len(classifier_tokens['positive'])} \\\n",
        "      \\nnegative tokens: {len(classifier_tokens['negative'])} \\\n",
        "      \\nneutral tokens: {len(classifier_tokens['neutral'])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYewkQ-KJmaN"
      },
      "source": [
        "And let's try making a prediction for a specific tweet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "My-ndiDjJmaN"
      },
      "outputs": [],
      "source": [
        "i = 100\n",
        "print(df.iloc[i,:])\n",
        "tweet2classify = df.iloc[i,:]['tokens']\n",
        "predict_tweet_sentiment(tweet2classify, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wISOvqofJmaN"
      },
      "source": [
        "Again, it is misclassified, but let's see whether the Lemmatization at performs better than the Stemming."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dI6Wm4kNJmaN"
      },
      "outputs": [],
      "source": [
        "df['predicted_sentiment'] = df['tokens'].apply(lambda x: predict_tweet_sentiment(x))\n",
        "df[['sentiment','predicted_sentiment','text', 'tokens']].head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0xMfQVQJmaN"
      },
      "outputs": [],
      "source": [
        "confusion_matrix(df['sentiment'], df['predicted_sentiment'])\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix(df['sentiment'], df['predicted_sentiment']), display_labels=['negative', 'neutral', 'positive'])\n",
        "disp.plot()\n",
        "mod_accuracy = accuracy_score(df['sentiment'], df['predicted_sentiment'])\n",
        "print(f\"model accuracy: {mod_accuracy*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sl9dxOaiJmaN"
      },
      "source": [
        "***\n",
        "#### Q4\n",
        "Even with the Stemming/Lemmatization, is it possible to use a proper machine learning model? For example, could we use linear regression, or even random forest? Why or why not? Explain by stating what the input to the model would be (or would need to be).\n",
        "\n",
        "`<input your answer here>`\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJ5Mxg5gJmaN"
      },
      "source": [
        "### 6: Vectorization\n",
        "\n",
        "To convert the tokens to numerical data we'll now use a form of vectorization. Let's first remove the previous predictions and then create a new column with the tokens joined together. We need to do this because the vectorization method works on the full text of each observation (not the list of tokens that we have been using up until now)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmnMsJWCJmaN"
      },
      "outputs": [],
      "source": [
        "if 'predicted_sentiment' in df.columns:\n",
        "    print(\"removing column, predicted_sentiment, from df\")\n",
        "    df.drop('predicted_sentiment', axis=1, inplace=True)\n",
        "\n",
        "df['textclean'] = df['tokens'].apply(lambda x: ' '.join(x))\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIqtjhVqJmaN"
      },
      "source": [
        "Let's also use a proper train-test split in order to get a better idea of how the model will perform on unseen data (we can still compare the training split to the results above to see how this approach works). We could use sklearn's train_test_split() function for this, but we'll do it manually here as a quick review."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IppvEf4QJmaO"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "random.seed(42)\n",
        "\n",
        "df_full = df.copy()\n",
        "assert df_full.shape[0] == 10000\n",
        "\n",
        "indices = list(range(df_full.shape[0]))\n",
        "\n",
        "random.shuffle(indices)\n",
        "\n",
        "train_indices = indices[:8500]\n",
        "test_indices = indices[8500:]\n",
        "\n",
        "# df will be the in-sample training dataset\n",
        "df = df_full.iloc[train_indices,:].copy()\n",
        "print(f\"df.shape: {df.shape}\")\n",
        "\n",
        "# df_test will be the out-of-sample validation dataset\n",
        "df_test = df_full.iloc[test_indices,:].copy()\n",
        "print(f\"df_test.shape: {df_test.shape}\")\n",
        "df_test.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceCECxCCJmaO"
      },
      "source": [
        "Now we will load the term-frequency inverse-document-frequency vectorizer from sklearn, `TfidfVectorizer`, to convert each tweet into a vector. We'll go ahead and call the resulting vectorized data, `X`, or `X_train` since it is only the training dataset. As with conventional statistical models, \"_X_\" represents the set of predictors, or independent variables.\n",
        "\n",
        "Also, note that `TfidfVectorizer` is a powerful text processing object. It has the ability to remove stop words, strip symbols, and do much of the work that our manual tokenization did. As such, we could easily use the original tweet text here, but we'll go ahead and continue with our manually tokenized data in the column, `textclean`.\n",
        "<a id=\"changetraining\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCNgOlvRJmaO"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_train = tfidf_vectorizer.fit_transform(df['textclean']).toarray()\n",
        "#X_train = tfidf_vectorizer.fit_transform(df['text']).toarray() # original tweet text (without our manual tokenization)\n",
        "\n",
        "print(f\"X_train.shape = {X_train.shape}\")\n",
        "type(X_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APG_LHMjJmaO"
      },
      "source": [
        "Note that the data type for `X_train` is no longer a pandas data frame. Instead, it is a numpy ndarray. We will talk much more about the python module, numpy, and its data types in the coming week. For now, let's just think of it a more efficently implemented object than a pandas data frame (e.g. smaller memory footprint), and one that we can think of a more of a matrix than a table of data. This can be seen by how we can easily index into `X_train` without the need for methods such as `iloc` or `loc`. Here we will look at the first 5 rows and 10 columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wt2rB-FMJmaO"
      },
      "outputs": [],
      "source": [
        "X_train[:5, :10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPZZYwOIJmaO"
      },
      "source": [
        "Looking at the output above, and back at the dimensions of `X_train` two cells back, what does this tell you about the data (aside from the size of the training data)? Specifically, what do the number of columns represent?\n",
        "\n",
        "If you're not sure, then look at the output of the following cell. We are looking at the first row of the training dataset, `X_train` again, but looking only at the non-zero values. These are the term-frequency inverse-document-frequency values for this tweet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qu6GWw24JmaP"
      },
      "outputs": [],
      "source": [
        "obs1 = list(X_train[0,:])\n",
        "for i, tfidf_val in enumerate(obs1):\n",
        "    if tfidf_val > 0:\n",
        "        print(f\"obs1 token at column i={i}, has a non-zero TF-IDF value: {tfidf_val}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEqfe1m4JmaP"
      },
      "source": [
        "If we wanted to, we could retrieve the token associated with each column of `X_train` by using the `tfidf_vectorizer`. Let's do that now just to see if we can try to recover the tweet. You'll notice that we can't recover the ordering of the words, but rather just the set of words that were in the tweet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZFmnBGTJmaP"
      },
      "outputs": [],
      "source": [
        "obs1 = list(X_train[0,:])\n",
        "for i, tfidf_val in enumerate(obs1):\n",
        "    if tfidf_val > 0:\n",
        "        print(f\"obs1 has a non-zero TF-IDF value: {tfidf_val} at col i={i} (associated with token: {tfidf_vectorizer.get_feature_names_out()[i]})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssPZt41EJmaP"
      },
      "source": [
        "We can verify this by going back to our training dataset, `df`, and checking what the first observation was. Looking at the output below. As stated above, we see that the same words are present in the data but not necessarily in the same order. Disregarding word order when vectorizing text data is sometimes (informally) referred to as a \"[__bag-of-words__](https://en.wikipedia.org/wiki/Bag-of-words_model)\" approach. Often times, \"__bag-of-words__\" suggests that just the counts (i.e. __term frequency__) is used, rather than what we are using here, __term-frequency inverse-document-frequency__. Nonetheless, the underlying idea that words frequency is accounted for, but word order is neglected, is an important observation to make here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7gYO87UaJmaP"
      },
      "outputs": [],
      "source": [
        "df.iloc[0,:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMaqlXkzJmaP"
      },
      "source": [
        "A good default model to start with that does not require too much fine tuning, and generally performs well is a [__Random Forest__](https://en.wikipedia.org/wiki/Random_forest) model. We won't dig into the details of how an RF model works exactly, but so long as we know that it creates many decision trees on random subsets of the data, then you've got a decent idea. Before doing so, we also need to convert the labels (i.e. `df.sentiment`) to a numpy data type. We'll quickly do that now and look at the first few observations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ao9r_yTDJmaP"
      },
      "outputs": [],
      "source": [
        "y_train = df.sentiment.to_numpy()\n",
        "y_train[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lutfTuHaJmaP"
      },
      "source": [
        "We'll not fit the random forest model on the training data and check the performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBHklSzcJmaP"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "model = RandomForestClassifier(n_estimators=200)\n",
        "model = model.fit(X_train, y_train)\n",
        "\n",
        "predictions_train = model.predict(X_train)\n",
        "predictions_train\n",
        "\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix(df['sentiment'], predictions_train), display_labels=['negative', 'neutral', 'positive'])\n",
        "disp.plot()\n",
        "print(f\"accuracy (on X_train): {accuracy_score(df['sentiment'], predictions_train):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbOQb-5HJmaP"
      },
      "source": [
        "Wow! Accuracy of ~99% is much better than what we saw with our ad hoc model. Let's check and see how well the model does on the test dataset now.\n",
        "\n",
        "Notice that we are using `tfidf_vectorizer.transform()` now, and not the same method we used before on the training dataset, which was `tfidf_vectorizer.fit_transform()`. The `fit_transform` method creates the vocabulary from the training data. The `transform` method will use the vocabulary that was previously made, but if it encounters a token in the test data that was never seen in the training data, then it will simply ignore it. This is one reason why making sure that training data for models is regularly updated.\n",
        "<a id=\"changetest\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dY0_bUGcJmaP"
      },
      "outputs": [],
      "source": [
        "df_test['textclean'] = df_test['tokens'].apply(lambda x: ' '.join(x))\n",
        "X_test = tfidf_vectorizer.transform(df_test['textclean']).toarray()\n",
        "#X_test = tfidf_vectorizer.transform(df_test['text']).toarray() # use original tweet text (without our tokenization)\n",
        "X_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDz3VNy9JmaP"
      },
      "outputs": [],
      "source": [
        "predictions_test = model.predict(X_test)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix(df_test['sentiment'], predictions_test), display_labels=['negative', 'neutral', 'positive'])\n",
        "disp.plot()\n",
        "print(f\"accuracy (on X_test): {accuracy_score(df_test['sentiment'], predictions_test):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5py7mBBWJmaP"
      },
      "source": [
        "Okay, the model performance on the test data is not nearly as good as the performance on the training dataset. That's expected to some degree.\n",
        "\n",
        "We may be able to improve it slightly by finding the optimal value of one of the random forest hyperparameters, e.g. `n_estimators`. However, one of the strengths of random forest is not it does a good job of overfitting. Knowing that, how do you think smaller values of `n_estimators` will affect accuracy on the test data? How do you think larger values will affect it? Try a few different values of `n_estimators`, both larger and smaller than the default of `n_estimators=100`, to see what happens.\n",
        "\n",
        "Below we go ahead and try many different hyperparameter values (for n_estimators) to see what the best value is. As can be seen, there is not an immediately obvious choice for the best value of the n_estimators hyperparameter, which is exactly why random forest is such a nice to model to use at the beginning. Admittedly, an RF model might not perform as well as some other machine learning models, but it is reliable and does a decent job at balancing the Bias-Variance Tradeoff right away without a lot of tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "567f_buKJmaP"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import log_loss\n",
        "\n",
        "# A function to create and fit a RF with a specific number of trees\n",
        "def fitRFModel(hyperparam_value):\n",
        "    rf_model = RandomForestClassifier(n_estimators=hyperparam_value, random_state=5)\n",
        "    rf_model.fit(X_train, y_train)\n",
        "    y_train_pred_prob = rf_model.predict_proba(X_train)\n",
        "    y_train_pred = rf_model.predict(X_train)\n",
        "    y_val_pred_prob = rf_model.predict_proba(X_test)\n",
        "    y_val_pred = rf_model.predict(X_test)\n",
        "    train_loss = log_loss(df['sentiment'], y_train_pred_prob, labels=['negative', 'neutral', 'positive'])\n",
        "    train_acc = accuracy_score(df['sentiment'], y_train_pred)\n",
        "    val_loss = log_loss(df_test['sentiment'], y_val_pred_prob, labels=['negative', 'neutral', 'positive'])\n",
        "    val_acc = accuracy_score(df_test['sentiment'], y_val_pred)\n",
        "    return (train_loss, val_loss, train_acc, val_acc)\n",
        "\n",
        "hyp_param_vals = list(range(5, 206, 50)) # possible values for n_estimators\n",
        "metrics = []\n",
        "for hp in hyp_param_vals:\n",
        "    metrics.append(fitRFModel(hp))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3RoS-wH3JmaP"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "fig = plt.figure(figsize=(6, 3))\n",
        "ax = fig.add_axes([0, 0, 1, 1]) #.1, 0.1, 0.8, 0.8]) # main axes\n",
        "ax.plot(hyp_param_vals, [metric[1] for metric in metrics], '--ro') # test loss\n",
        "ax.plot(hyp_param_vals, [metric[0] for metric in metrics], '--bo') # training loss\n",
        "ax.legend([\"Test Loss\", \"Train Loss\"], loc=1)\n",
        "ax.set_xticks(hyp_param_vals)\n",
        "ax.set(xlabel=\"n_estimators\", ylabel=\"loss (lower is better)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AmFO7kBKJmaP"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(6, 3))\n",
        "ax = fig.add_axes([0, 0, 1, 1]) #.1, 0.1, 0.8, 0.8]) # main axes\n",
        "ax.plot(hyp_param_vals, [metric[3] for metric in metrics], '--ro') # test accuracy\n",
        "ax.plot(hyp_param_vals, [metric[2] for metric in metrics], '--bo') # training accuracy\n",
        "ax.legend([\"Test Accuracy\", \"Train Accuracy\"], loc=4)\n",
        "ax.set_xticks(hyp_param_vals)\n",
        "ax.set(xlabel=\"n_estimators\", ylabel=\"accuracy (higher is better)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgQx47hWJmaP"
      },
      "source": [
        "***\n",
        "#### Q5\n",
        "What happens if we don't use the the `textclean` column (which was based on the lemmatized tokens and stopword removal), but instead use the original `text` column? Remember that you need to change this not just for the [training data](#changetraining), but also for the [test data](#changetest). Then, we want to know specifically...\n",
        "\n",
        "Q5a: How many more (or fewer) input features are there when using TFIDF on the original (un-lemmatized) `text` column?\n",
        "\n",
        "`<input your answer here>`\n",
        "\n",
        "Q5b: Is the accuracy better or worse when using the original `text` column?\n",
        "\n",
        "`<input your answer here>`\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFcvvQ3WJmaP"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "py313",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}